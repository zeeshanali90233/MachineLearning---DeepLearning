{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e574600",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Lowercasing\n",
    "### 2Ô∏è‚É£ Removing URLs / Links\n",
    "### 3Ô∏è‚É£ Removing Numbers / Digits\n",
    "### 4Ô∏è‚É£ Removing Punctuation\n",
    "### 5Ô∏è‚É£ Tokenization\n",
    "### 6Ô∏è‚É£ Stopword Removal\n",
    "### 7Ô∏è‚É£ Lemmatization (or Stemming)\n",
    "### 8Ô∏è‚É£ Removing Extra Whitespace\n",
    "### 9Ô∏è‚É£ Optionally: Handling Emojis / Special Characters\n",
    "### üîü Optionally: Removing Rare or Short Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383324b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090d1727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wulvob get your medircations online qnb ikud v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>computer connection from cnn com wednesday es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>university degree obtain a prosperous future m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>thanks for all your answers guys i know i shou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  ounce feather bowl hummingbird opec moment ala...\n",
       "1      1  wulvob get your medircations online qnb ikud v...\n",
       "2      0   computer connection from cnn com wednesday es...\n",
       "3      1  university degree obtain a prosperous future m...\n",
       "4      0  thanks for all your answers guys i know i shou..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('combined_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03d86683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\786\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\786\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\786\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\786\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\786\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\786\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1d1fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample cleaned text:\n",
      " 0    ounce feather bowl hummingbird opec moment ala...\n",
      "1    wulvob get medircations online qnb ikud viagra...\n",
      "2    computer connection cnn com wednesday escapenu...\n",
      "3    university degree obtain prosperous future mon...\n",
      "4    thanks answer guy know checked rsync manual wo...\n",
      "Name: clean_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()                          # lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)   # remove links\n",
    "    text = re.sub(r\"\\d+\", \"\", text)              # remove numbers\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# 3Ô∏è‚É£ Tokenize, remove stopwords, and lemmatize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 4Ô∏è‚É£ Apply preprocessing\n",
    "data[\"clean_text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nSample cleaned text:\\n\", data[\"clean_text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "941e6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b446e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1500)\n",
    "X = vectorizer.fit_transform(data[\"clean_text\"])\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e1f48a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9721390053924506\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      7938\n",
      "           1       0.97      0.98      0.97      8752\n",
      "\n",
      "    accuracy                           0.97     16690\n",
      "   macro avg       0.97      0.97      0.97     16690\n",
      "weighted avg       0.97      0.97      0.97     16690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7Ô∏è‚É£ Train simple classifier\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8Ô∏è‚É£ Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a485ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_email = \"\"\"\n",
    "Congratulations! You have won a $1000 Walmart gift card.\n",
    "Click the link below to claim your prize now!\n",
    "\"\"\"\n",
    "\n",
    "cleaned_email = preprocess_text(new_email)\n",
    "\n",
    "X_new = vectorizer.transform([cleaned_email])\n",
    "print(X_new.toarray())\n",
    "# Predict\n",
    "prediction = model.predict(X_new)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c297523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14474cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì© The email is: SPAM\n"
     ]
    }
   ],
   "source": [
    "# Show result\n",
    "if prediction == 1:\n",
    "    print(\"üì© The email is: SPAM\")\n",
    "else:\n",
    "    print(\"üì® The email is: NOT SPAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f361d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Email: Win a free vacation to Dubai! Click here to register.\n",
      "‚Üí Prediction: SPAM\n",
      "\n",
      "Email: Hey John, can we meet tomorrow about the project?\n",
      "‚Üí Prediction: NOT SPAM\n",
      "\n",
      "Email: Limited offer!!! Get cheap meds online now!\n",
      "‚Üí Prediction: SPAM\n",
      "\n",
      "Email: Please find attached the report for Q3 results.\n",
      "‚Üí Prediction: NOT SPAM\n"
     ]
    }
   ],
   "source": [
    "emails = [\n",
    "    \"Win a free vacation to Dubai! Click here to register.\",\n",
    "    \"Hey John, can we meet tomorrow about the project?\",\n",
    "    \"Limited offer!!! Get cheap meds online now!\",\n",
    "    \"Please find attached the report for Q3 results.\"\n",
    "]\n",
    "\n",
    "for e in emails:\n",
    "    pred = model.predict(vectorizer.transform([preprocess_text(e)]))[0]\n",
    "    label = \"SPAM\" if pred == 1 else \"NOT SPAM\"\n",
    "    print(f\"\\nEmail: {e}\\n‚Üí Prediction: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea50706",
   "metadata": {},
   "source": [
    "### Exporting the Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b891aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features: 1500\n",
      "\n",
      "Sample Vocabulary Words:\n",
      " ['ability' 'able' 'ac' 'accept' 'access' 'according' 'account'\n",
      " 'acquisition' 'acrobat' 'across' 'act' 'action' 'activity' 'actual'\n",
      " 'actually' 'ad' 'add' 'added' 'addition' 'additional']\n"
     ]
    }
   ],
   "source": [
    "# Total number of features\n",
    "print(\"Total Features:\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "# Show first 20 words learned\n",
    "print(\"\\nSample Vocabulary Words:\\n\", vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "943ff7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save\n",
    "joblib.dump(model, \"spam_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b3565",
   "metadata": {},
   "source": [
    "### We can now only have to reuse them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1f247c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Later load them\n",
    "model = joblib.load(\"spam_model.pkl\")\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
